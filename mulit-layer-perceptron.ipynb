{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0824853b",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587bdf3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29fc2d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5346b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0176b4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d67d6a5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a75d6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import ssl\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten \n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Embedding \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.layers import LSTM, SpatialDropout1D, Bidirectional, Dropout\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.constraints import max_norm\n",
    "from keras.callbacks import EarlyStopping\n",
    "import string\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1306b2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download's the stop word's\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c114ae",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c900c9e3",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c47e6ca",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dp_train = dataset['train'].to_pandas()\n",
    "dp_test = dataset['test'].to_pandas()\n",
    "\n",
    "#used so we don't have to use the entire training dataset\n",
    "dp_sample = dp_train.sample(n=100000, random_state=42)\n",
    "dp_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f40d6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocess's the text data\n",
    "#by removing stop words and leading spaces \n",
    "#this data pre-process was inspired by: https://www.kaggle.com/code/gcdatkin/gru-hotel-rating-prediction\n",
    "def pre_process_data(X):\n",
    "    stop_words = stopwords.words('english')\n",
    "    X = re.sub(r'\\d+', ' ', X)\n",
    "    X = X.split()\n",
    "    X = \" \".join([word for word in X if word.lower().strip() not in stop_words])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057965f4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dp_sample['text'] = dp_sample['text'].apply(pre_process_data)\n",
    "dp_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8aa7ca",
   "metadata": {},
   "source": [
    "## Nesseccary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b6334",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#provides the option a toke/sequence embedding approach\n",
    "#takes the modified reviews\n",
    "#return the modified inputs and the amount of words's to be used in an embedding layer\n",
    "#sentences are the actual reviews\n",
    "#num_words are the amount of top words_taken\n",
    "#use_entire_vocab, boolean to determine if we want to use top words or everything\n",
    "#max_length, determines if he want to take into account the max_length sequnce or average length sequnce\n",
    "def create_sequences(sentences, num_words, use_entire_vocab, max_length_voacb):\n",
    "    #takes the most used word's so we don't have the entire vocab\n",
    "    if use_entire_vocab:\n",
    "        tokenizer = Tokenizer()\n",
    "    else:\n",
    "        tokenizer = Tokenizer(num_words = num_words)\n",
    "    \n",
    "   \n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    if use_entire_vocab:\n",
    "        num_words = len(tokenizer.word_index)\n",
    "    \n",
    "    if max_length_voacb:\n",
    "        #get the length of the largest sequnces\n",
    "        max_length = np.max(list(map(lambda x: len(x), sequences)))\n",
    "    else:\n",
    "        sequence_lengths = [len(seq) for seq in sequences]\n",
    "        # Calculate the average length\n",
    "        max_length = int(sum(sequence_lengths) / len(sequence_lengths))\n",
    "    \n",
    "    \n",
    "    #pad all the input's to be the same length of the max length\n",
    "    #print(max_length)\n",
    "    inputs = pad_sequences(sequences, maxlen=max_length, padding = 'post')\n",
    "    \n",
    "    return inputs, num_words, max_length\n",
    "\n",
    "#used to test word to vec embeedings weights on the neural network\n",
    "def make_embeddings(data, num_words):\n",
    "    sentence_split = [line.split() for line in data]\n",
    "    \n",
    "    word2vec_model = Word2Vec(sentences=sentence_split, vector_size=100, window=5, min_count=3, workers=4)   \n",
    "    vocab = list(word2vec_model.wv.key_to_index.keys())\n",
    "    print(len(vocab))\n",
    "    # Convert tokens to embeddings\n",
    "    embedding_matrix = np.zeros((len(vocab), word2vec_model.vector_size))\n",
    "    for index, word in enumerate(vocab):\n",
    "        embedding_matrix[index] = word2vec_model.wv[word]\n",
    "        \n",
    "    \n",
    "    indexed_data = []\n",
    "    for sentence in sentence_split:\n",
    "        indexed_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in word2vec_model.wv.key_to_index:\n",
    "                #indexed_sentence = [word2vec_model.wv.key_to_index[word] for word in sentence]\n",
    "                indexed_sentence.append(word2vec_model.wv.key_to_index[word])\n",
    "        indexed_data.append(indexed_sentence)\n",
    "    max_length = np.max(list(map(lambda x: len(x), indexed_data)))\n",
    "    padded_data = pad_sequences(indexed_data, maxlen=max_length, padding='post')\n",
    "    return embedding_matrix, len(vocab), padded_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45afc17b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs, num_words, max_length = create_sequences(dp_sample['text'],50000, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87e204",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(num_words)\n",
    "print(max_length)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7047f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make the testing and training parititons\n",
    "#inputs are tokenized seqeuences and data is the pandas dataframe used for testing\n",
    "def make_training_partition(inputs, data):\n",
    "    labels = np.array(data['label'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputs, labels, train_size=0.75, random_state=42)\n",
    "    y_train = tf.one_hot(y_train, depth=5)\n",
    "    y_train = tf.cast(y_train, dtype=tf.int32)\n",
    "    y_test =  tf.one_hot(y_test, depth=5)\n",
    "    y_test = tf.cast(y_test, dtype=tf.int32)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#performs predictions on the model\n",
    "#model is the current model to be tested\n",
    "#X_test is the testing partition\n",
    "#y_test is is the label testing partition\n",
    "def predict(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions_transformed = np.argmax(predictions, axis=1)\n",
    "    test_transformed = np.argmax(y_test, axis=1)\n",
    "    print(classification_report(predictions_transformed, test_transformed))\n",
    "    \n",
    "    #check how many were being predicted one away\n",
    "    classified_correct = 0\n",
    "    classified_one_away = 0\n",
    "    classified_more_than_one = 0\n",
    "    for i in range(0, len(predictions_transformed)):\n",
    "        if predictions_transformed[i] == test_transformed[i]:\n",
    "            classified_correct = classified_correct + 1\n",
    "        else:\n",
    "            if abs(predictions_transformed[i] - test_transformed[i]) == 1:\n",
    "                classified_one_away  = classified_one_away + 1\n",
    "            else:\n",
    "                classified_more_than_one = classified_more_than_one + 1\n",
    "    print(\"Number of Correct Classificaiton: \" + str(classified_correct))\n",
    "    print(\"Number of Incorrect by One: \" + str(classified_one_away))\n",
    "    print(\"Number of Incorrect More than One: \" + str(classified_more_than_one))\n",
    "    \n",
    "    cm = confusion_matrix(predictions_transformed, test_transformed)\n",
    "    ConfusionMatrixDisplay(cm).plot()\n",
    "    \n",
    "\n",
    "#make's a baseline neural network with an embedding layer\n",
    "#num_words are the \n",
    "def make_baseline_model(num_words, max_length, X_train):\n",
    "    model = Sequential([\n",
    "        Embedding(num_words,128,input_length=X_train.shape[1]),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        Dense(8,activation='relu'),\n",
    "        #output layer\n",
    "        Dense(5, activation = 'softmax'), ])\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy', Precision(), Recall()])\n",
    "    return model\n",
    "\n",
    "#train a neural network with word2vec embedding weights\n",
    "#reviews are the cleaned data as used above\n",
    "def word2vec_models(data):\n",
    "    embedding_matrix, vocab_size, inputs = make_embeddings(data, 50000)\n",
    "    X_train, X_test, y_train, y_test = make_training_partition(inputs, dp_sample)\n",
    "    model = Sequential([\n",
    "         Embedding(vocab_size, 100, weights = [embedding_matrix]),\n",
    "         tf.keras.layers.GlobalAveragePooling1D(),\n",
    "         Dense(8,activation='relu'),\n",
    "         Dense(5, activation = 'softmax'),\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy', Precision(), Recall()])\n",
    "    model.fit(X_train,y_train ,epochs=8, validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=3)])\n",
    "    return model, X_test, y_test \n",
    "\n",
    "#Long Short term memory network with no embedding\n",
    "#num_words are the number of words in the vocabulary\n",
    "#X_train is the training dataset\n",
    "def lstm_test(num_words, X_train, y_train):\n",
    "    regularise = tf.keras.regularizers.l2(0.001)\n",
    "    model = Sequential([\n",
    "        Embedding(num_words,128,input_length=X_train.shape[1]),\n",
    "        Dropout(0.5),\n",
    "        LSTM(32,kernel_constraint=max_norm(3)),\n",
    "        Dense(32,activation='relu',kernel_regularizer=regularise),\n",
    "        Dropout(0.5),\n",
    "        Dense(5,activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    #Fitting the model\n",
    "    history1 =  model.fit(X_train,y_train ,epochs=8, validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=3)])\n",
    "    return model\n",
    "\n",
    "\n",
    "def lstm_test_with_embed(data):\n",
    "    embedding_matrix, vocab_size, inputs = make_embeddings(data, 50000)\n",
    "    X_train, X_test, y_train, y_test = make_training_partition(inputs, dp_sample)\n",
    "    model = Sequential([\n",
    "         Embedding(vocab_size, 100, weights = [embedding_matrix]),\n",
    "         Bidirectional(LSTM(128, dropout=0.2)),\n",
    "         Dense(128, activation = 'relu'),\n",
    "         Dropout(0.5),\n",
    "         Dense(64, activation = 'relu'),\n",
    "         Dense(5,activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),metrics=['accuracy'])\n",
    "    #Fitting the model\n",
    "    history1 =  model.fit(X_train,y_train ,epochs=8, validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=3)])\n",
    "    return model, X_test, y_test\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a08928",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1c0b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = make_training_partition(inputs, dp_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810914bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the baseline model on the specified number of samples\n",
    "baseline_model = make_baseline_model(num_words, max_length, X_train)\n",
    "#Fitting the model\n",
    "baseline_history = baseline_model.fit(X_train,y_train ,epochs=8, validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad91d59",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prediction on the baseline model\n",
    "predict(baseline_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1c5fc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training on the word_embedding model\n",
    "word_embed_model, X_test_embed, Y_test_embed = word2vec_models(dp_sample['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425fb2b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict(word_embed_model, X_test_embed, Y_test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d0037",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train on the lstm model\n",
    "lstm_model = lstm_test(num_words, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3cc3a5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predictions on the LSTM model\n",
    "predict(lstm_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074d463",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM with word embedding\n",
    "lstm_embed, X_embed_ltest, y_embed_ltest = lstm_test_with_embed(dp_sample['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51065c9a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict(lstm_embed, X_embed_ltest, y_embed_ltest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85ff3c",
   "metadata": {},
   "source": [
    "## Filtered Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e97a5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's remove the second and fourth start\n",
    "df_sample_filtered = dp_sample[~dp_sample['label'].isin([1, 3])]\n",
    "df_sample_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687dde7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sample_filtered['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344bf18",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sample_filtered['text'] = df_sample_filtered['text'].apply(pre_process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea87045",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sample_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147fdbf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_inputs, filtered_words, max_length = create_sequences(df_sample_filtered['text'],50000, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdac5ff",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_fil, X_test_fil, y_train_fil, y_test_fil = make_training_partition(filtered_inputs, df_sample_filtered)\n",
    "y_train_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dae0c6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_model = lstm_test(filtered_words, X_train_fil, y_train_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32c808",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict(filter_model, X_test_fil, y_test_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3ce61",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
